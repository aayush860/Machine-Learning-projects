{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Shipment and delivery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did nothing for me, didn't help lost even with...</td>\n",
       "      <td>Useless</td>\n",
       "      <td>Not Effective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have bought these bags and  immediately open...</td>\n",
       "      <td>TRASH!!! Do not buy these bags it’s a waist of...</td>\n",
       "      <td>Customer Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gave me an allergic reaction on my face :(</td>\n",
       "      <td>Do not recommend</td>\n",
       "      <td>Allergic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>These don't compare to the name brand wipes. F...</td>\n",
       "      <td>Can't tackle big messes</td>\n",
       "      <td>Texture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  Did nothing for me, didn't help lost even with...   \n",
       "1  Did nothing for me, didn't help lost even with...   \n",
       "2  I have bought these bags and  immediately open...   \n",
       "3         Gave me an allergic reaction on my face :(   \n",
       "4  These don't compare to the name brand wipes. F...   \n",
       "\n",
       "                                        Review Title                  topic  \n",
       "0                                            Useless  Shipment and delivery  \n",
       "1                                            Useless          Not Effective  \n",
       "2  TRASH!!! Do not buy these bags it’s a waist of...       Customer Service  \n",
       "3                                   Do not recommend               Allergic  \n",
       "4                            Can't tackle big messes                Texture  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "review = pd.read_csv('/Users/aayushbhargava/Downloads/Dataset/train.csv')\n",
    "review_test = pd.read_csv('/Users/aayushbhargava/Downloads/Dataset/test.csv')\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n",
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check if any row is null\n",
    "print (review.columns[review.isnull().any()])\n",
    "print (review_test.columns[review_test.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shipment and delivery', 'Not Effective', 'Customer Service',\n",
       "       'Allergic', 'Texture', 'Quality/Contaminated', 'Color and texture',\n",
       "       'Bad Taste/Flavor', 'Too big to swallow', 'Smells Bad',\n",
       "       'Too Sweet', 'Ingredients', 'Expiry', 'Packaging',\n",
       "       'Wrong Product received', 'Pricing', 'False Advertisement',\n",
       "       'Inferior to competitors', \"Didn't Like\", 'Customer Issues',\n",
       "       'Hard to Chew'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review['topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data (removing stopwords, punctuations or whatever, converting to lower case)\n",
    "def fc(tab_name):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    def cleanerr(tbc):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for num,x in enumerate(tab_name[tbc]):\n",
    "            x=re.sub(r'[^\\w\\s]','',x)\n",
    "            word_tokens = word_tokenize(x)\n",
    "            filtered_sentence = [stemmer.stem(w.lower()) for w in word_tokens if not w in stop_words and len(w)>=3]\n",
    "            tab_name[tbc].iloc[num]=filtered_sentence\n",
    "            tab_name[tbc].iloc[num]=\" \".join(tab_name[tbc].iloc[num])\n",
    "    cleanerr('Review Text')\n",
    "    cleanerr('Review Title')\n",
    "    tab_name.head()\n",
    "\n",
    "    tab_name['Review Text']=tab_name['Review Text']+' '+tab_name['Review Title']\n",
    "    tab_name['Review Text'].iloc[0]\n",
    "\n",
    "fc(review)\n",
    "fc(review_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(5959,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8507    this wors protein ive ever tri the tast horrib...\n",
       "8508    veri small easi swallow flavor left stomach kn...\n",
       "8509    veri small easi swallow flavor left stomach kn...\n",
       "8510    good increas bad cholesterol ldlbr doesnt suit...\n",
       "8511                will buy powder thicklik past consist\n",
       "Name: Review Text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg=review['Review Text']\n",
    "print(type(gg))\n",
    "#gg.append(review_test['Review Text'], ignore_index=True)\n",
    "gg=pd.concat([gg,review_test['Review Text']], ignore_index=True)\n",
    "print (review['Review Text'].shape)\n",
    "gg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8737"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# create a count vectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "# fit the count vectorizer using the text data\n",
    "count_vectorizer.fit(gg)\n",
    "# collect the vocabulary items used in the vectorizer\n",
    "dictionary = count_vectorizer.vocabulary_.items() \n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(\"english\")\n",
    "tfid_vectorizer.fit(gg)\n",
    "tfid_matrix = tfid_vectorizer.transform(review['Review Text'])\n",
    "array_text = tfid_matrix.todense()\n",
    "\n",
    "tfid_matrix = tfid_vectorizer.transform(review_test['Review Text'])\n",
    "array_text_test = tfid_matrix.todense()\n",
    "\n",
    "#To inverse_transform label encoders\n",
    "le = LabelEncoder()\n",
    "rto = le.fit_transform(review['topic'])\n",
    "labels=np.array(rto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be used to save and retrive array for faster access\n",
    "'''\n",
    "np.save('/root/Documents/rev_text.npy', array_text)\n",
    "np.save('/root/Documents/rev_title.npy', array_title)\n",
    "np.save('/root/Documents/labels.npy', labels)\n",
    "\n",
    "array_text = np.load('/root/Documents/rev_text.npy')\n",
    "array_title = np.load('/root/Documents/rev_title.npy')\n",
    "labels = np.load('/root/Documents/labels.npy')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=int(array_text.shape[0]*0.90)\n",
    "traffic_lab=array_text[:r]\n",
    "label_lab=labels[:r]\n",
    "\n",
    "traffic_val=array_text[r:]\n",
    "label_val=labels[r:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5363, 8737)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_class=5, n_estimators=100,\n",
      "              n_jobs=1, nthread=None, objective='multi:softmax', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "[0]\tvalidation_0-merror:0.446309\n",
      "Will train until validation_0-merror hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-merror:0.447987\n",
      "[2]\tvalidation_0-merror:0.447987\n",
      "[3]\tvalidation_0-merror:0.447987\n",
      "[4]\tvalidation_0-merror:0.444631\n",
      "[5]\tvalidation_0-merror:0.442953\n",
      "[6]\tvalidation_0-merror:0.441275\n",
      "[7]\tvalidation_0-merror:0.442953\n",
      "[8]\tvalidation_0-merror:0.441275\n",
      "[9]\tvalidation_0-merror:0.439597\n",
      "[10]\tvalidation_0-merror:0.437919\n",
      "[11]\tvalidation_0-merror:0.436242\n",
      "[12]\tvalidation_0-merror:0.436242\n",
      "[13]\tvalidation_0-merror:0.437919\n",
      "[14]\tvalidation_0-merror:0.437919\n",
      "[15]\tvalidation_0-merror:0.437919\n",
      "[16]\tvalidation_0-merror:0.437919\n",
      "Stopping. Best iteration:\n",
      "[11]\tvalidation_0-merror:0.436242\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a1c11176fb2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_lab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_lab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraffic_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0maccuracy_lgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_lgbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb=XGBClassifier(objective='multi:softmax',learning_rate=0.01,n_class=5, max_depth=2)\n",
    "print (xgb)\n",
    "xgb.fit(traffic_lab,label_lab,eval_set = [(traffic_val, label_val)],verbose=True,early_stopping_rounds=5)\n",
    "tt=xgb.predict(traffic_val)\n",
    "accuracy_lgbm = accuracy_score(tt,label_val)\n",
    "print (accuracy_lgbm)\n",
    "\n",
    "#filename = '/root/Documents/xgboot_model.sav'\n",
    "#joblib.dump(lgbm, filename)\n",
    "\n",
    "for i in range(1,20):\n",
    "    tt=(xgb.predict(traffic_val[i]))\n",
    "    print (str(label_val[i])+'\\t'+str(tt))\n",
    "\n",
    "for i in range(1,20):\n",
    "    tt=np.argsort(-xgb.predict_proba(traffic_val[i]))\n",
    "    print (str(label_val[i])+'\\t'+str(tt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[0]\n",
      "9\t[14]\n",
      "12\t[12]\n",
      "19\t[0]\n",
      "14\t[14]\n",
      "0\t[14]\n",
      "15\t[15]\n",
      "11\t[10]\n",
      "10\t[10]\n",
      "17\t[17]\n",
      "1\t[17]\n",
      "18\t[18]\n",
      "10\t[18]\n",
      "1\t[1]\n",
      "1\t[1]\n",
      "4\t[1]\n",
      "5\t[1]\n",
      "14\t[1]\n",
      "1\t[1]\n",
      "11\t[1]\n",
      "1\t[1]\n",
      "4\t[1]\n",
      "10\t[0]\n",
      "0\t[0]\n",
      "0\t[0]\n",
      "14\t[14]\n",
      "11\t[14]\n",
      "14\t[1]\n",
      "1\t[1]\n",
      "1\t[1]\n",
      "17\t[17]\n",
      "17\t[14]\n",
      "14\t[14]\n",
      "13\t[0]\n",
      "15\t[15]\n",
      "11\t[11]\n",
      "19\t[0]\n",
      "14\t[1]\n",
      "1\t[1]\n",
      "9\t[9]\n",
      "17\t[17]\n",
      "1\t[1]\n",
      "0\t[0]\n",
      "11\t[11]\n",
      "19\t[12]\n",
      "17\t[12]\n",
      "14\t[12]\n",
      "11\t[12]\n",
      "12\t[12]\n",
      "17\t[6]\n",
      "6\t[6]\n",
      "4\t[4]\n",
      "11\t[1]\n",
      "10\t[10]\n",
      "17\t[17]\n",
      "17\t[1]\n",
      "0\t[1]\n",
      "1\t[1]\n",
      "17\t[19]\n",
      "19\t[19]\n",
      "0\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "9\t[[14  0 11  1 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "12\t[[12  4  0 11  1 15 10 17 19 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "19\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "14\t[[14  0 11  1 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "0\t[[14  0 11  1 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "15\t[[15 20  4  0 11  1 12 10 17 19 14 13 16  2 18  5  9  6  7  8  3]]\n",
      "11\t[[10  0 11  1 12 15 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "10\t[[10  0 11  1 12 15 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[17 15  1  0 11 12 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "1\t[[17 15  1  0 11 12 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "18\t[[18  0 11  1 12 15 10 17 19 14 13 16  2  4  5 20  9  6  7  8  3]]\n",
      "10\t[[18  0 11  1 12 15 10 17 19 14 13 16  2  4  5 20  9  6  7  8  3]]\n",
      "1\t[[ 1  0 11  5 12 15 10 17 19 14 13 16  2  4 18 20  9  6  7  8  3]]\n",
      "1\t[[ 1  5  4  0 11 12 15 10 17 19 14 13 16  2 18 20  9  6  7  8  3]]\n",
      "4\t[[ 1  5  4  0 11 12 15 10 17 19 14 13 16  2 18 20  9  6  7  8  3]]\n",
      "5\t[[ 1  5  4  0 11 12 15 10 17 19 14 13 16  2 18 20  9  6  7  8  3]]\n",
      "14\t[[ 1 14  0 11 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "1\t[[ 1 14  0 11 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "11\t[[ 1 19 11  4  0 12 15 10 17 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "1\t[[ 1 19 11  4  0 12 15 10 17 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "4\t[[ 1 19 11  4  0 12 15 10 17 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "10\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "0\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "0\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "14\t[[14  0 11  1 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "11\t[[14  0 11  1 12 15 10 17 19 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "14\t[[ 1 14  9  0 11  5 12 15 10 17 19 13 16  2  4 18 20  6  7  3  8]]\n",
      "1\t[[ 1 14  9  0 11  5 12 15 10 17 19 13 16  2  4 18 20  6  7  3  8]]\n",
      "1\t[[ 1  4  0 11 12 15 10 17 19 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "17\t[[17  0 11  1 12 15 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[14 17 16  0 11  1 12 15 10 19 13  2  4 18  5 20  9  6  7  3  8]]\n",
      "14\t[[14 17 16  0 11  1 12 15 10 19 13  2  4 18  5 20  9  6  7  3  8]]\n",
      "13\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "15\t[[15  0 11  1 12 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "11\t[[11  0  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "19\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "14\t[[ 1 13 14  9  0 11 12 15 10 17 19 16  2  4 18  5 20  6  7  3  8]]\n",
      "1\t[[ 1 13 14  9  0 11 12 15 10 17 19 16  2  4 18  5 20  6  7  3  8]]\n",
      "9\t[[ 9  0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  6  7  8  3]]\n",
      "17\t[[17 20  4  0 11  1 12 15 10 19 14 13 16  2 18  5  9  6  7  3  8]]\n",
      "1\t[[ 1  2  0 19 11 12 15 10 17 14 13 16  4 18  5 20  9  6  7  8  3]]\n",
      "0\t[[ 0 11  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "11\t[[11  0  1 12 15 10 17 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "19\t[[12 19 17 11  0  1 15 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[12 19 17 11  0  1 15 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "14\t[[12 19 17 11  0  1 15 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "11\t[[12 19 17 11  0  1 15 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "12\t[[12 19 17 11  0  1 15 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[ 6 17  0 11  1 12 15 10 19 14 13 16  2  4 18  5 20  9  7  8  3]]\n",
      "6\t[[ 6 17  0 11  1 12 15 10 19 14 13 16  2  4 18  5 20  9  7  8  3]]\n",
      "4\t[[ 4  0 11  1 12 15 10 17 19 14 13 16  2 18  5 20  9  6  7  8  3]]\n",
      "11\t[[ 1 11 14  0 12  9 15 10 17 19 13 16  2  4 18  5 20  6  7  8  3]]\n",
      "10\t[[10 16  0 11  1 12 15 17 19 14 13  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[17 15  0 11  1 12 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[ 1 17 11  0 12 15 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "0\t[[ 1 17 11  0 12 15 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "1\t[[ 1 17 11  0 12 15 10 19 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "17\t[[19 17  0 15 11  1 12 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n",
      "19\t[[19 17  0 15 11  1 12 10 14 13 16  2  4 18  5 20  9  6  7  8  3]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,60):\n",
    "    tt=(xgb.predict(traffic_val[i]))\n",
    "    print (str(label_val[i])+'\\t'+str(tt))\n",
    "\n",
    "for i in range(0,60):\n",
    "    tt=np.argsort(-xgb.predict_proba(traffic_val[i]))\n",
    "    print (str(label_val[i])+'\\t'+str(tt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Allergic']\n",
      "['Ingredients']\n",
      "['Ingredients']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Shipment and delivery']\n",
      "['Not Effective']\n",
      "['Packaging']\n",
      "['Allergic']\n",
      "['Allergic']\n",
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Not Effective']\n",
      "['Not Effective']\n",
      "['Bad Taste/Flavor']\n",
      "['Allergic']\n",
      "['Color and texture']\n",
      "['Color and texture']\n",
      "['Customer Service']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Packaging']\n",
      "['Not Effective']\n",
      "['Not Effective']\n",
      "['Quality/Contaminated']\n",
      "['Quality/Contaminated']\n",
      "['Packaging']\n",
      "['Packaging']\n",
      "['Packaging']\n",
      "['Packaging']\n",
      "['Packaging']\n",
      "['Expiry']\n",
      "['Allergic']\n",
      "['Allergic']\n",
      "['Allergic']\n",
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Shipment and delivery']\n",
      "['Shipment and delivery']\n",
      "['Allergic']\n",
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Wrong Product received']\n",
      "['Wrong Product received']\n",
      "['Packaging']\n",
      "['Allergic']\n",
      "['Allergic']\n",
      "['Expiry']\n",
      "['Expiry']\n",
      "['Bad Taste/Flavor']\n",
      "['Bad Taste/Flavor']\n",
      "['Not Effective']\n",
      "['Bad Taste/Flavor']\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0,59):\n",
    "        tt=(xgb.predict(array_text_test[i]))\n",
    "        print (str(le.inverse_transform(tt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================All Done\n"
     ]
    }
   ],
   "source": [
    "#for i in range(0,10):\n",
    "#    tt=np.argsort(-xgb.predict_proba(array_text_test[i]))\n",
    "#    print (str(tt))\n",
    "import csv\n",
    "\n",
    "f = open(\"/Users/aayushbhargava/Downloads/Dataset/Submission.csv\", \"a\")\n",
    "fff=csv.writer(f)\n",
    "fff.writerow(['Review Text','Review Title','topic'])\n",
    "count=0\n",
    "for i in range(0,len(array_text_test)):\n",
    "    try:\n",
    "        if (array_text_test[i]==array_text_test[i+1]).all():\n",
    "            tt=np.argsort(-xgb.predict_proba(array_text_test[i]))\n",
    "            tt=(str(le.inverse_transform([tt[0][count]])))\n",
    "            fff.writerow([review_test['Review Text'].iloc[i],review_test['Review Title'].iloc[i], str(tt).replace(\"['\",'').replace(\"']\",'')])\n",
    "            count=count+1\n",
    "        else:\n",
    "            if count==0:\n",
    "                tt=(xgb.predict(array_text_test[i]))\n",
    "                fff.writerow([review_test['Review Text'].iloc[i],review_test['Review Title'].iloc[i],str(le.inverse_transform(tt)).replace(\"['\",'').replace(\"']\",'')])\n",
    "            else:\n",
    "                tt=np.argsort(-xgb.predict_proba(array_text_test[i]))\n",
    "                tt=(str(le.inverse_transform([tt[0][count]])))\n",
    "                fff.writerow([review_test['Review Text'].iloc[i],review_test['Review Title'].iloc[i],str(tt).replace(\"['\",'').replace(\"']\",'')])\n",
    "                count=0\n",
    "    except:\n",
    "        tt=(xgb.predict(array_text_test[i]))\n",
    "        fff.writerow([review_test['Review Text'].iloc[i],review_test['Review Title'].iloc[i],str(le.inverse_transform(tt)).replace(\"['\",'').replace(\"']\",'')])\n",
    "f.close()\n",
    "print ('==================All Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
